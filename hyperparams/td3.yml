# Tuned
MountainCarContinuous-v0:
  normalize: True
  n_timesteps: 30000
  policy: 'MlpPolicy'
  learning_rate: !!float 3e-4
  buffer_size: 50000
  batch_size: 256
  n_episodes_rollout: -1
  gradient_steps: 8
  train_freq: 8
  learning_starts: 0
  use_sde: True
  policy_kwargs: "dict(log_std_init=0.0, net_arch=[64, 64])"

Pendulum-v0:
  n_timesteps: 20000
  policy: 'MlpPolicy'
  buffer_size: 200000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

LunarLanderContinuous-v2:
  n_timesteps: !!float 3e5
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

BipedalWalker-v2:
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

# To be tuned
BipedalWalkerHardcore-v2:
  n_timesteps: !!float 1e7
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 500000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3, use_expln=True)"
  use_sde: True

# Tuned
HalfCheetahBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

AntBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

HopperBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

Walker2DBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

# TO BE tested
HumanoidBulletEnv-v0:
  normalize: True
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  gamma: 0.99
  buffer_size: 1000000
  # noise_type: 'normal'
  # noise_std: 0.1
  learning_starts: 10000
  batch_size: 100
  learning_rate: !!float 1e-3
  train_freq: 128
  gradient_steps: 128
  n_episodes_rollout: -1
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3, lr_sde=1e-3, clip_noise=None)"
  use_sde: True
  # policy_kwargs: "dict(net_arch=[400, 300])"

ReacherBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.99
  buffer_size: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  learning_starts: 10000
  batch_size: 100
  learning_rate: !!float 1e-3
  train_freq: 1000
  gradient_steps: 1000
  policy_kwargs: "dict(net_arch=[400, 300])"

InvertedDoublePendulumBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

InvertedPendulumSwingupBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.98
  buffer_size: 200000
  learning_starts: 10000
  train_freq: 64
  gradient_steps: 64
  n_episodes_rollout: -1
  learning_rate: !!float 6e-4
  policy_kwargs: "dict(net_arch=[400, 300], log_std_init=-3.62, lr_sde=1.5e-3)"
  use_sde: True

MinitaurBulletEnv-v0:
  env_wrapper: utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  gamma: 0.99
  buffer_size: 1000000
  noise_type: 'normal'
  noise_std: 0.1
  learning_starts: 10000
  batch_size: 100
  learning_rate: !!float 1e-3
  train_freq: 1000
  gradient_steps: 1000
  policy_kwargs: "dict(net_arch=[400, 300])"
