# === Real Robot envs
NeckGoalEnvRelativeSparse-v2:
  model_class: 'sac'
  # env_wrapper:
  #   - utils.wrappers.HistoryWrapper:
  #       horizon: 2
  #   - utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 100000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  n_episodes_rollout: 1
  gradient_steps: -1
  train_freq: -1
  # 10 episodes of warm-up
  learning_starts: 1500
  use_sde_at_warmup: True
  use_sde: True
  sde_sample_freq: 64
  policy_kwargs: "dict(log_std_init=-2, net_arch=[256, 256])"
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  online_sampling: False

NeckGoalEnvRelativeDense-v2:
  model_class: 'sac'
  env_wrapper:
    - utils.wrappers.HistoryWrapperObsDict:
        horizon: 2
  #   - utils.wrappers.TimeFeatureWrapper
  n_timesteps: !!float 1e6
  policy: 'MlpPolicy'
  learning_rate: !!float 7.3e-4
  buffer_size: 200000
  batch_size: 256
  ent_coef: 'auto'
  gamma: 0.99
  tau: 0.02
  n_episodes_rollout: 1
  gradient_steps: -1
  train_freq: -1
  # 10 episodes of warm-up
  learning_starts: 1500
  use_sde_at_warmup: True
  use_sde: True
  sde_sample_freq: 64
  policy_kwargs: "dict(log_std_init=-2, net_arch=[256, 256])"
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  online_sampling: False

# DDPG hyperparams
#parking-v0:
#  n_timesteps: !!float 2e5
#  policy: 'MlpPolicy'
#  model_class: 'ddpg'
#  n_sampled_goal: 4
#  goal_selection_strategy: 'future'
#  buffer_size: 1000000
#  batch_size: 256
#  gamma: 0.95
#  learning_rate: !!float 1e-3
#  noise_type: 'normal'
#  noise_std: 0.2
#  policy_kwargs: "dict(net_arch=[256, 256, 256])"
#  online_sampling: True
#  max_episode_length: 100


# SAC hyperparams, her paper
parking-v0:
  n_timesteps: !!float 2e5
  policy: 'MlpPolicy'
  model_class: 'sac'
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  buffer_size: 1000000
  batch_size: 256
  gamma: 0.95
  learning_rate: !!float 1e-3
  # noise_type: 'normal'
  # noise_std: 0.2
  policy_kwargs: "dict(net_arch=[256, 256, 256])"
  online_sampling: False
  # normalize: True
  max_episode_length: 100

# TD3 hyperparams, her paper
#parking-v0:
#  n_timesteps: !!float 2e5
#  policy: 'MlpPolicy'
#  model_class: 'td3'
#  n_sampled_goal: 4
#  goal_selection_strategy: 'future'
#  buffer_size: 1000000
#  batch_size: 256
#  gamma: 0.95
#  learning_rate: !!float 1e-3
#  noise_type: 'normal'
#  noise_std: 0.2
#  policy_kwargs: "dict(net_arch=[256, 256, 256])"
#  online_sampling: True
#  max_episode_length: 100


# Mujoco Robotic Env
# DDPG hyperparams
# FetchReach-v1:
#   n_timesteps: !!float 20000
#   policy: 'MlpPolicy'
#   model_class: 'ddpg'
#   n_sampled_goal: 4
#   goal_selection_strategy: 'future'
#   buffer_size: 1000000
#   batch_size: 256
#   gamma: 0.95
#   random_exploration: 0.3
#   actor_lr: !!float 1e-3
#   critic_lr: !!float 1e-3
#   noise_type: 'normal'
#   noise_std: 0.2
#   normalize_observations: true
#   normalize_returns: false
#   policy_kwargs: "dict(layers=[256, 256, 256])"
#   online_sampling: True

# NOTE: shoube be run with 8 workers: mpirun -n 8
# FetchPush-v1:
#   n_timesteps: !!float 2e6
#   policy: 'MlpPolicy'
#   model_class: 'ddpg'
#   n_sampled_goal: 4
#   goal_selection_strategy: 'future'
#   buffer_size: 200000
#   batch_size: 256
#   gamma: 0.95
#   random_exploration: 0.3
#   actor_lr: !!float 1e-3
#   critic_lr: !!float 1e-3
#   noise_type: 'normal'
#   noise_std: 0.2
#   normalize_observations: true
#   normalize_returns: false
#   policy_kwargs: "dict(layers=[16, 16, 16])"

FetchPush-v1:
  env_wrapper:
    - utils.wrappers.HistoryWrapperObsDict:
        horizon: 2
    # - utils.wrappers.TimeFeatureObsDictWrapper
  n_timesteps: !!float 3e6
  policy: 'MlpPolicy'
  model_class: 'sac'
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  buffer_size: 1000000
  ent_coef: 'auto'
  gamma: 0.95
  learning_rate: !!float 7e-4
  use_sde: True
  gradient_steps: -1
  train_freq: -1
  n_episodes_rollout: 1
  sde_sample_freq: 10
  # noise_type: 'normal'
  # noise_std: 0.2
  learning_starts: 1000
  online_sampling: True
  normalize: True

FetchPickAndPlace-v1:
  n_timesteps: !!float 4e6
  policy: 'MlpPolicy'
  model_class: 'sac'
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  buffer_size: 1000000
  ent_coef: 'auto'
  # batch_size: 256
  gamma: 0.95
  # learning_rate: !!float 1e-3
  learning_starts: 1000
  train_freq: 1
  online_sampling: True

# SAC hyperparams
FetchReach-v1:
  n_timesteps: !!float 20000
  policy: 'MlpPolicy'
  model_class: 'sac'
  n_sampled_goal: 4
  goal_selection_strategy: 'future'
  buffer_size: 1000000
  ent_coef: 'auto'
  batch_size: 256
  gamma: 0.95
  learning_rate: 0.001
  learning_starts: 1000
  online_sampling: True
  normalize: True


# TD3 hyperparams
# FetchReach-v1:
#   n_timesteps: !!float 25000
#   policy: 'MlpPolicy'
#   model_class: 'td3'
#   n_sampled_goal: 4
#   goal_selection_strategy: 'future'
#   buffer_size: 1000000
#   batch_size: 256
#   gamma: 0.95
#   learning_rate: 0.001
#   learning_starts: 1000
