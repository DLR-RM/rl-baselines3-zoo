# Tuned
CartPole-v1:
  n_envs: 1
  n_timesteps: !!float 1e5
  policy: 'LinearPolicy'
  pop_size: 4
  n_top: 2

# Tuned
Pendulum-v1: &pendulum-params
  n_envs: 1
  n_timesteps: !!float 2e6
  policy: 'MlpPolicy'
  normalize: "dict(norm_obs=True, norm_reward=False)"
  pop_size: 8
  n_top: 4
  policy_kwargs: "dict(net_arch=[16])"

# Tuned
LunarLander-v2:
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  pop_size: 32
  n_top: 6
  policy_kwargs: "dict(net_arch=[])"

# Tuned
LunarLanderContinuous-v2:
  <<: *pendulum-params
  n_timesteps: !!float 2e6
  pop_size: 16
  n_top: 4

# Tuned
Acrobot-v1:
  <<: *pendulum-params
  n_timesteps: !!float 5e5

# Tuned
MountainCar-v0:
  <<: *pendulum-params
  pop_size: 16
  n_timesteps: !!float 5e5

# Tuned
MountainCarContinuous-v0:
  <<: *pendulum-params
  n_timesteps: !!float 5e5

# === Pybullet Envs ===
# To be tuned
HalfCheetahBulletEnv-v0: &pybullet-defaults
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  pop_size: 64
  n_top: 6
  extra_noise_std: 0.1

# To be tuned
AntBulletEnv-v0:
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.02
  delta_std: !!float 0.03
  n_delta: 32
  n_top: 32
  alive_bonus_offset: 0
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[128, 64])"
  zero_policy: False


Walker2DBulletEnv-v0:
  policy: 'MlpPolicy'
  n_timesteps: !!float 7.5e7
  learning_rate: !!float 0.03
  delta_std: !!float 0.025
  n_delta: 40
  n_top: 30
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[64, 64])"
  zero_policy: False

# Tuned
HopperBulletEnv-v0:
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  pop_size: 64
  n_top: 6
  extra_noise_std: 0.1
  alive_bonus_offset: -1

ReacherBulletEnv-v0:
  <<: *pybullet-defaults
  n_timesteps: !!float 1e6

# === Mujoco Envs ===
# Tuned
Swimmer-v3:
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  n_top: 2

Hopper-v3:
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  pop_size: 64
  n_top: 6
  extra_noise_std: 0.1
  alive_bonus_offset: -1


HalfCheetah-v3:
  <<: *pendulum-params
  n_timesteps: !!float 1e6
  pop_size: 50
  n_top: 6
  extra_noise_std: 0.1

Walker2d-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  pop_size: 40
  n_top: 30
  alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"

# TO BE TUNED
Ant-v3:
  <<: *pendulum-params
  # For hyperparameter optimization, to alive_bonus_offset
  # taken into account (reward_offset):
  # env_wrapper:
  #   - utils.wrappers.DoneOnSuccessWrapper:
  #         reward_offset: -1.0
  #   - stable_baselines3.common.monitor.Monitor
  n_timesteps: !!float 2e6
  pop_size: 64
  n_top: 6
  # extra_noise_std: 0.1
  # noise_multiplier: 0.999
  alive_bonus_offset: -1
  policy_kwargs: "dict(net_arch=[])"


Humanoid-v3:
  n_envs: 1
  policy: 'LinearPolicy'
  n_timesteps: !!float 2.5e8
  pop_size: 256
  n_top: 256
  alive_bonus_offset: -5
  normalize: "dict(norm_obs=True, norm_reward=False)"

BipedalWalker-v3:
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 1e8
  pop_size: 64
  n_top: 32
  alive_bonus_offset: -0.1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[16])"

# TO Be Tuned
BipedalWalkerHardcore-v3:
  n_envs: 1
  policy: 'MlpPolicy'
  n_timesteps: !!float 5e8
  pop_size: 64
  n_top: 32
  alive_bonus_offset: -0.1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  policy_kwargs: "dict(net_arch=[16])"

A1Walking-v0:
  <<: *pendulum-params
  n_timesteps: !!float 2e6

A1Jumping-v0:
  policy: 'LinearPolicy'
  n_timesteps: !!float 7.5e7
  pop_size: 80
  n_top: 30
  # alive_bonus_offset: -1
  normalize: "dict(norm_obs=True, norm_reward=False)"
  # policy_kwargs: "dict(net_arch=[16])"
