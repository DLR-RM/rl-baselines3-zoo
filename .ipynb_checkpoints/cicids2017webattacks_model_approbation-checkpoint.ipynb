{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import  ConfusionMatrixDisplay\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from pcwcf.common.policies import ConstrainedActorCriticPolicy\n",
    "from gym_pcwcf.config.config_cicids2017_webattacks import FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dirname):\n",
    "    # Create log dir\n",
    "    import shutil\n",
    "    eval_log_dir = \"/tmp/gym/val/\"\n",
    "    try:\n",
    "        shutil.rmtree(eval_log_dir)\n",
    "    except:\n",
    "        pass\n",
    "    finally:\n",
    "        os.makedirs(eval_log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'gym_pcwcf:cwcf-v0'\n",
    "env_kwargs = dict(mode='TEST',\n",
    "                 is_binary_classification=True,\n",
    "                 random_mode=False,\n",
    "                 lambda_coefficient=0.05,\n",
    "                 terminal_reward=[[0, -0.3], \n",
    "                                  [-0.7, 0]])\n",
    "monitor_kwargs = dict(info_keywords=('y_true', 'y_predict', 'features'))\n",
    "eval_env = gym.make(env_id, **env_kwargs)\n",
    "eval_env = Monitor(eval_env, filename=eval_log_dir, **monitor_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(eval_env.data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(eval_env.data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '/home/student/projects/site-packages/rl-baselines3-zoo/logs/ppo/cwcf-v0_12/best_model.zip'\n",
    "model = PPO.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=len(eval_env.data_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, std_reward "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = load_results(eval_log_dir)\n",
    "print(np.mean(df.y_true.values==df.y_predict.values))\n",
    "print(np.mean(df.r))\n",
    "print(np.mean(df.l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df.l.map(lambda x: x-1), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast, itertools\n",
    "features_union = list(itertools.chain.from_iterable(df.features.map(ast.literal_eval)))\n",
    "# features_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "features_counts = Counter(features_union)\n",
    "n = 20\n",
    "y = [c for f, c in features_counts.most_common(n)]\n",
    "x = [FEATURES[f] for f, c in features_counts.most_common(n)]\n",
    "plt.bar(x,y)\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.features.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(classification_report(df.y_true, df.y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "disp = ConfusionMatrixDisplay.from_predictions(df.y_true, df.y_predict)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "print(f\"Confusion matrix:\\n{disp.confusion_matrix}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env.reset()\n",
    "for a in np.arange(5,1, -1):\n",
    "    eval_env.step(a)\n",
    "print(eval_env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(df.y_true, df.y_predict)\n",
    "with open('logs/lambda_1e-2_val.txt', 'w') as report_file:\n",
    "    report_file.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wc -l /home/student/projects/site-packages/rl-baselines3-zoo/logs/ppo/report*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv( '/home/student/projects/site-packages/rl-baselines3-zoo/logs/ppo/report_cwcf-v0_500-trials-1024000-tpe-median_1639590587.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr().sort_values(by='value', key=np.abs, ascending=False).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='value', ascending=False)[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "baseline",
   "language": "python",
   "name": "baseline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
